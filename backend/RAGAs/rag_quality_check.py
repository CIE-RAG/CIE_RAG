import os
import sys
import json
import random
import requests
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from evaluation_metrics import LocalRAGMetrics, evaluate_single
# Dynamically add the backend directory to Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from response_generator.generator import ResponseGenerator
from response_generator.retriever import Retriever
from ingestion.faiss_database import setup_faiss_with_text_storage
## Note to Team: These two functions: cieRAG_pipeline1, cieRAG_pipeline2 are the only ones that needs a fix
# What this file does: Loads Queries_groundtruth.json as DF, samples some questions out of it, and runs ragas metrics on this. 
# No need to change evaluation_metrics.py -- review iff needed 
# Note to team: Check out this url pipeline and correct at points
def cieRAG_pipeline1(query, username="default_user"):
    # Define the API endpoint for the first pipeline
    url = "http://localhost:8001/chat"  # Assuming a different endpoint for pipeline1
    payload = {
        "query": query,
        "username": username
    }
    try:
        response = requests.post(url, json=payload)
        if response.status_code == 200:
            response_data = response.json()
            return response_data["response"]
        else:
            response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")
        return "Sorry, I encountered an error while processing your request."

# Initialize once
retriever = Retriever() # Note to Team: Please do replace these lines if applicable
retriever_with_faiss = setup_faiss_with_text_storage(retriever)
generator2 = ResponseGenerator(use_reranker=True)
generator2.load_faiss(retriever_with_faiss)

def cieRAG_pipeline2(query, top_k=5, return_full=False):
    """
    Local version of CIE RAG Pipeline 2 using ResponseGenerator class.
    - query: input question
    - return_full: if True, return full metadata (answer, sources, etc.)
    """
    try:
        result = generator2.generate(query, top_k=top_k)
        if return_full:
            return result
        return result.get("answer", "No answer generated.")
    except Exception as e:
        print(f"❌ Error in cieRAG_pipeline2: {e}")
        return "Sorry, I encountered an error while processing your request."


def cieRAG_pipeline(query, username="default_user"):
    """
    This function attempts to run cieRAG_pipeline1 and falls back to cieRAG_pipeline2 if an error occurs.

    Args:
        query (str): The query to be processed.
        username (str): The username associated with the query (default is "default_user").

    Returns:
        str: The response generated by either of the RAG pipelines.
    """
    try:
        # Attempt to run cieRAG_pipeline1
        response = cieRAG_pipeline1(query, username)
        return response
    except Exception as e:
        print(f"cieRAG_pipeline1 failed with error: {e}. Trying cieRAG_pipeline2...")
        try:
            # If cieRAG_pipeline1 fails, attempt to run cieRAG_pipeline2
            response = cieRAG_pipeline2(query, username)
            return response
        except Exception as e:
            # If both pipelines fail, raise an error
            raise Exception(f"Both cieRAG_pipeline1 and cieRAG_pipeline2 failed. Error in cieRAG_pipeline2: {e}")

# Use the combined function in your batchEvaluate function
def batchEvaluate(sampleDF, output_file):
    """Batch Evaluate the sample dataset"""
    print(f"Starting the batch evaluation of {len(sampleDF)} queries")
    df = sampleDF.copy()
    results = []
    # Initialize metrics calculator
    metrics_calc = LocalRagMetrics()
    print(f"Processing queries...")
    for idx, row in df.iterrows():
        try:
            query = row["query"]
            ground_truth = row["ground_truth"]
            # Show progress
            print(f"[{idx+1}/{len(df)}] Processing: '{query[:50]}...'")
            # Get RAG pipeline response using the combined function
            response = combined_cieRAG_pipeline(query)
            # Calculate metrics
            scores = metrics_calc.evaluate_all(query, ground_truth, response)
            # Combine all data
            result = {
                "query": query,
                "ground_truth": ground_truth,
                "RAG_pipeline_response": response,
                **scores
            }
            results.append(result)
        except Exception as e:
            print(f"❌ Error processing query {idx+1}: {e}")
            # Add failed result with NaN metrics
            result = {
                "query": row["query"],
                "ground_truth": row["ground_truth"],
                "RAG_pipeline_response": f"Error: {str(e)}",
                "context_relevance": np.nan,
                "context_precision": np.nan,
                "context_recall": np.nan,
                "answer_relevance": np.nan,
                "answer_correctness": np.nan
            }
            results.append(result)

    # Create results DataFrame
    result_df = pd.DataFrame(results)
    # Calculate and display summary
    print(f"\nBatch Evaluation Complete!")
    print(f"Successfully processed: {len(result_df) - result_df['context_relevance'].isna().sum()}/{len(result_df)} queries")
    # Summary statistics (excluding NaN values)
    metric_columns = ["context_relevance", "context_precision", "context_recall",
                     "answer_relevance", "answer_correctness"]
    print(f"\nSummary Statistics:")
    print("-" * 50)
    for metric in metric_columns:
        if metric in result_df.columns:
            valid_scores = result_df[metric].dropna()
            if len(valid_scores) > 0:
                mean_score = valid_scores.mean()
                std_score = valid_scores.std()
                print(f"{metric:20s}: {mean_score:.3f} (±{std_score:.3f})")
    # Save results
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"batch_evaluation_results_{timestamp}.csv"
    result_df.to_csv(output_file, index=False)
    print(f"\nResults saved to: {output_file}")
    return result_df


# The rest of your code remains the same

def cieRAG_pipeline2(query, username="default_user"):
    # Define the API endpoint
    url = "http://localhost:8000/chat"

    # Prepare the request payload
    payload = {
        "query": query,
        "username": username
    }

    try:
        # Send a POST request to the API
        response = requests.post(url, json=payload)

        # Check if the request was successful
        if response.status_code == 200:
            # Parse the JSON response
            response_data = response.json()
            return response_data["response"]
        else:
            # Handle errors returned by the API
            response.raise_for_status()
    except requests.exceptions.RequestException as e:
        # Handle any exceptions that occur during the request
        print(f"An error occurred: {e}")
        return "Sorry, I encountered an error while processing your request."

def load_dataset(filename: str, sample_size: int = 20, random_seed: int = 42):
    "Loads Dataset and returns a random sample"
    try:
        with open(filename, "r", encoding="utf-8") as f:
            raw_data = json.load(f)

        # Setting random seed:
        random.seed(random_seed)
        np.random.seed(random_seed)
        rawDF = pd.DataFrame(raw_data)

        # Sample Random Rows:
        if len(rawDF) > sample_size:
            sampledDF = rawDF.sample(n=sample_size, random_state=42)
            print(f"Sampled {sample_size} rows from Question Bank of length {len(rawDF)}")
        else:
            sampledDF = rawDF

        print(f"Dataset Loaded Successfully")
        print(f"Sample Queries loaded from sampled Dataset")
        return sampledDF
    except FileNotFoundError:
        print(f"{filename} not found. Creating Empty Dataset.")
        return pd.DataFrame()


def batchEvaluate(sampleDF, output_file=None):
    """Batch Evaluate the sample dataset using `evaluate_single()`"""
    print(f"Starting the batch evaluation of {len(sampleDF)} queries")

    df = sampleDF.copy()
    results = []

    print(f"Processing queries...")
    for idx, row in df.iterrows():
        try:
            query = row["query"]
            ground_truth = row["ground_truth"]

            print(f"[{idx+1}/{len(df)}] Processing: '{query[:50]}...'")

            # Get response from your RAG pipeline
            response = cieRAG_pipeline(query)

            # (Optional) fetch or simulate the contexts used in retrieval
            contexts = row.get("contexts", [])  # or retrieve_context(query)

            # Evaluate using helper
            result_df = evaluate_single(query, ground_truth, response, contexts)
            result_dict = result_df.iloc[0].to_dict()

            result_dict["ground_truth"] = ground_truth
            result_dict["RAG_pipeline_response"] = response
            results.append(result_dict)

        except Exception as e:
            print(f"❌ Error processing query {idx+1}: {e}")
            results.append({
                "query": row["query"],
                "ground_truth": row["ground_truth"],
                "RAG_pipeline_response": f"Error: {str(e)}",
                "context_relevance": np.nan,
                "context_precision": np.nan,
                "context_recall": np.nan,
                "answer_relevance": np.nan,
                "answer_correctness": np.nan
            })

    result_df = pd.DataFrame(results)

    # Summary
    print(f"\nBatch Evaluation Complete!")
    print(f"Successfully processed: {len(result_df) - result_df['context_relevance'].isna().sum()}/{len(result_df)} queries")

    metric_columns = ["context_relevance", "context_precision", "context_recall",
                      "answer_relevance", "answer_correctness"]

    print(f"\nSummary Statistics:")
    print("-" * 50)
    for metric in metric_columns:
        if metric in result_df.columns:
            valid_scores = result_df[metric].dropna()
            if len(valid_scores) > 0:
                mean_score = valid_scores.mean()
                std_score = valid_scores.std()
                print(f"{metric:20s}: {mean_score:.3f} (±{std_score:.3f})")

    # Save file
    if output_file is None:
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"batch_evaluation_results_{timestamp}.csv"
    result_df.to_csv(output_file, index=False)
    print(f"\n✅ Results saved to: {output_file}")

    return result_df

def ragStrength(filename: str, output_file: str, sample_size: int = 20):
    sampled_df = load_dataset(filename, sample_size, random_seed=42)
    result_df = batchEvaluate(sampled_df, output_file)

ragStrength("./Queries_groundtruth.json", "ragas_first_run.csv", 10)


# # Batch Processing Function:
# def batchEvaluate(sampleDF, output_file):
#     """Batch Evaluate the sample dataset"""
#     print(f"Starting the batch evaluation of {len(sampleDF)} queries")

#     df = sampleDF.copy()
#     results = []

#     # Initialize metrics calculator
#     metrics_calc = LocalRagMetrics()

#     print(f"Processing queries...")
#     for idx, row in df.iterrows():
#         try:
#             query = row["query"]
#             ground_truth = row["ground_truth"]

#             # Show progress
#             print(f"[{idx+1}/{len(df)}] Processing: '{query[:50]}...'")

#             # Get RAG pipeline response
#             response = cieRAG_pipeline(query)

#             # Calculate metrics
#             scores = metrics_calc.evaluate_all(query, ground_truth, response)

#             # Combine all data
#             result = {
#                 "query": query,
#                 "ground_truth": ground_truth,
#                 "RAG_pipeline_response": response,
#                 **scores
#             }
#             results.append(result)

#         except Exception as e:
#             print(f"❌ Error processing query {idx+1}: {e}")
#             # Add failed result with NaN metrics
#             result = {
#                 "query": row["query"],
#                 "ground_truth": row["ground_truth"],
#                 "RAG_pipeline_response": f"Error: {str(e)}",
#                 "context_relevance": np.nan,
#                 "context_precision": np.nan,
#                 "context_recall": np.nan,
#                 "answer_relevance": np.nan,
#                 "answer_correctness": np.nan
#             }
#             results.append(result)

#     # Create results DataFrame
#     result_df = pd.DataFrame(results)

#     # Calculate and display summary
#     print(f"\nBatch Evaluation Complete!")
#     print(f"Successfully processed: {len(result_df) - result_df['context_relevance'].isna().sum()}/{len(result_df)} queries")

#     # Summary statistics (excluding NaN values)
#     metric_columns = ["context_relevance", "context_precision", "context_recall",
#                      "answer_relevance", "answer_correctness"]

#     print(f"\nSummary Statistics:")
#     print("-" * 50)
#     for metric in metric_columns:
#         if metric in result_df.columns:
#             valid_scores = result_df[metric].dropna()
#             if len(valid_scores) > 0:
#                 mean_score = valid_scores.mean()
#                 std_score = valid_scores.std()
#                 print(f"{metric:20s}: {mean_score:.3f} (±{std_score:.3f})")

#     # Save results
#     timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
#     output_file = f"batch_evaluation_results_{timestamp}.csv"
#     result_df.to_csv(output_file, index=False)
#     print(f"\n Results saved to: {output_file}")

#     return result_df